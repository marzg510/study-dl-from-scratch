{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから作るDeep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4章 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 学習アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 2層ニューラルネットワークのクラス\n",
    "\n",
    "2層のニューラルネットワーク（隠れ層が1層のニューラルネットワーク）を対象に、MNISTデータセットを使って学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/gotowork/workspace/study-dl-from-scratch/deep-learning-from-scratch/ch04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd ../deep-learning-from-scratch/ch04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    # このクラスで使用する変数\n",
    "    ## params : ニューラルネットワークのパラメータを保持するディクショナリ変数（インスタンス変数）\n",
    "    ##          params['W1']は1層目の重み、params['b1']は1層目のバイアス。\n",
    "    ##          params['W2']は2層目の重み、params['b2']は2層目のバイアス。\n",
    "    #\n",
    "    ## grads : 勾配を保持するディクショナリ変数（numerical_gradient()メソッドの返り値）\n",
    "    ##         grads['W1']は1層目の重みの勾配、grads['b1']は1層目のバイアスの勾配。\n",
    "    ##         grads['W2']は2層目の重みの勾配、grads['b2']は2層目のバイアスの勾配。\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一つ例を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # 今の層（入力層）のニューロン数×次の層（隠れ層）ニューロン数の行列になる\n",
    "print(net.params['b1'].shape) # 次の層（隠れ層）のニューロン数の行列になる\n",
    "print(net.params['W2'].shape) # 今の層（隠れ層）のニューロン数×次の層（出力層）のニューロン数の行列になる\n",
    "print(net.params['b2'].shape) # 次の層（出力層）のニューロン数の行列になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推論処理の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # ダミーの入力データ（１００枚分）\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05656624, 0.19617819, 0.10486648, ..., 0.42354662, 0.35273118,\n",
       "        0.87174438],\n",
       "       [0.57755882, 0.48298945, 0.21353107, ..., 0.38059549, 0.23743903,\n",
       "        0.33146339],\n",
       "       [0.46275798, 0.02757556, 0.42526475, ..., 0.39976671, 0.15884461,\n",
       "        0.55388627],\n",
       "       ...,\n",
       "       [0.13981253, 0.78022038, 0.24436076, ..., 0.94848046, 0.21790573,\n",
       "        0.32274977],\n",
       "       [0.39965894, 0.56755264, 0.24788565, ..., 0.28554599, 0.77706347,\n",
       "        0.87499767],\n",
       "       [0.50886433, 0.67234244, 0.37372261, ..., 0.51165776, 0.82388759,\n",
       "        0.78253123]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0959692 , 0.09629166, 0.09979841, 0.09871939, 0.11098569,\n",
       "        0.101662  , 0.09740745, 0.10014477, 0.10529902, 0.09372241],\n",
       "       [0.0960181 , 0.09635296, 0.09954321, 0.09883072, 0.11141286,\n",
       "        0.10196294, 0.09756556, 0.09961232, 0.10519653, 0.09350482],\n",
       "       [0.09610602, 0.09622185, 0.09965249, 0.09874341, 0.111017  ,\n",
       "        0.10171183, 0.09757746, 0.10022934, 0.10529124, 0.09344936],\n",
       "       [0.0960031 , 0.09620877, 0.09979786, 0.09865434, 0.11064326,\n",
       "        0.10164532, 0.09758329, 0.10009401, 0.10551626, 0.0938538 ],\n",
       "       [0.09573872, 0.09627769, 0.09979617, 0.09875095, 0.11092637,\n",
       "        0.10193495, 0.09753129, 0.10013841, 0.10541893, 0.09348653],\n",
       "       [0.09608266, 0.09623521, 0.09996819, 0.09863595, 0.11083231,\n",
       "        0.10199436, 0.09737278, 0.10024432, 0.10516339, 0.09347084],\n",
       "       [0.0962095 , 0.09621766, 0.0997767 , 0.09886011, 0.11125865,\n",
       "        0.10178827, 0.09723453, 0.100102  , 0.10513272, 0.09341986],\n",
       "       [0.09611028, 0.09621453, 0.09931993, 0.09852109, 0.11102311,\n",
       "        0.10186059, 0.09778156, 0.10014316, 0.10544091, 0.09358486],\n",
       "       [0.09614926, 0.09617313, 0.09965823, 0.09842515, 0.11089886,\n",
       "        0.10188489, 0.09768633, 0.0999343 , 0.10542004, 0.09376982],\n",
       "       [0.09622079, 0.09641348, 0.09994284, 0.09831357, 0.11068983,\n",
       "        0.10183874, 0.09771748, 0.10004215, 0.10545106, 0.09337006],\n",
       "       [0.09597318, 0.09619068, 0.09959599, 0.09850834, 0.11121816,\n",
       "        0.10168085, 0.09765469, 0.10020872, 0.10554742, 0.09342196],\n",
       "       [0.09593744, 0.09607142, 0.09974204, 0.09864149, 0.1109025 ,\n",
       "        0.10172469, 0.09765638, 0.10019478, 0.1055715 , 0.09355776],\n",
       "       [0.09561446, 0.09594464, 0.0995904 , 0.09851793, 0.11102418,\n",
       "        0.10173884, 0.09777992, 0.10027942, 0.10584145, 0.09366876],\n",
       "       [0.09558378, 0.0958398 , 0.10004239, 0.09863142, 0.11137283,\n",
       "        0.10194819, 0.09718729, 0.10037185, 0.1055093 , 0.09351316],\n",
       "       [0.09585444, 0.09627405, 0.09958736, 0.09934802, 0.1112339 ,\n",
       "        0.10209414, 0.09727784, 0.09997786, 0.10480145, 0.09355094],\n",
       "       [0.09611598, 0.09613269, 0.09995682, 0.09870592, 0.11099764,\n",
       "        0.10186484, 0.09753594, 0.09985947, 0.10525992, 0.09357077],\n",
       "       [0.09604281, 0.09615939, 0.09980191, 0.09871553, 0.11078038,\n",
       "        0.10194442, 0.09730491, 0.09983538, 0.10562679, 0.09378848],\n",
       "       [0.09619551, 0.09632717, 0.09959315, 0.09861105, 0.11075337,\n",
       "        0.1016708 , 0.09776394, 0.10036016, 0.10515977, 0.0935651 ],\n",
       "       [0.09551494, 0.09584579, 0.09961565, 0.09887012, 0.11066842,\n",
       "        0.10153649, 0.09833531, 0.10029537, 0.10579424, 0.09352367],\n",
       "       [0.09591568, 0.09587945, 0.09990013, 0.09857249, 0.11098718,\n",
       "        0.10183507, 0.09753802, 0.10014086, 0.10563296, 0.09359817],\n",
       "       [0.09629656, 0.09647731, 0.09950417, 0.09859348, 0.11098733,\n",
       "        0.10181913, 0.09742704, 0.10033922, 0.10527937, 0.0932764 ],\n",
       "       [0.09562757, 0.09609371, 0.09985753, 0.09882521, 0.11105402,\n",
       "        0.1019724 , 0.09796424, 0.10004377, 0.10501743, 0.09354412],\n",
       "       [0.09596794, 0.09639043, 0.09975831, 0.09870121, 0.11134237,\n",
       "        0.10195358, 0.09734183, 0.09988719, 0.10497292, 0.09368422],\n",
       "       [0.0956564 , 0.09600067, 0.10014795, 0.09892562, 0.11085386,\n",
       "        0.10191743, 0.09729664, 0.10018645, 0.1056187 , 0.09339627],\n",
       "       [0.09619236, 0.09614339, 0.09975819, 0.09848814, 0.11107975,\n",
       "        0.10162632, 0.09744223, 0.09976957, 0.10563783, 0.09386222],\n",
       "       [0.09595962, 0.09622195, 0.09966432, 0.09842423, 0.11111064,\n",
       "        0.10186266, 0.09747285, 0.10037959, 0.10523325, 0.09367089],\n",
       "       [0.09611353, 0.09613726, 0.09962854, 0.09847807, 0.1108223 ,\n",
       "        0.10154539, 0.09775637, 0.10009913, 0.10574838, 0.09367102],\n",
       "       [0.09577227, 0.09656172, 0.09985729, 0.09830164, 0.11106193,\n",
       "        0.10220295, 0.09732817, 0.10006921, 0.10533653, 0.09350828],\n",
       "       [0.09639014, 0.09640208, 0.09987472, 0.09868489, 0.11088305,\n",
       "        0.10173901, 0.09732782, 0.09991419, 0.10541862, 0.09336547],\n",
       "       [0.09642533, 0.0963643 , 0.0994125 , 0.0988011 , 0.11097859,\n",
       "        0.10189693, 0.09741494, 0.10003662, 0.10517313, 0.09349655],\n",
       "       [0.09587946, 0.09618479, 0.0999441 , 0.09869959, 0.11110515,\n",
       "        0.10169749, 0.09732329, 0.10032891, 0.10521602, 0.09362118],\n",
       "       [0.09585156, 0.09629584, 0.09966101, 0.09873037, 0.11129734,\n",
       "        0.10143278, 0.09765847, 0.09996351, 0.10546251, 0.0936466 ],\n",
       "       [0.0963045 , 0.09604901, 0.09965888, 0.09919298, 0.11100481,\n",
       "        0.10226653, 0.09740773, 0.09991793, 0.10477093, 0.0934267 ],\n",
       "       [0.09597218, 0.09606379, 0.09988107, 0.09874731, 0.11120515,\n",
       "        0.10204365, 0.09751483, 0.09982527, 0.10508043, 0.09366633],\n",
       "       [0.09564994, 0.09638073, 0.09975319, 0.09866704, 0.11095188,\n",
       "        0.10162285, 0.09764271, 0.09988113, 0.10583416, 0.09361637],\n",
       "       [0.0960714 , 0.09616274, 0.09971743, 0.09873967, 0.11077833,\n",
       "        0.10173065, 0.09751468, 0.10002157, 0.10557509, 0.09368845],\n",
       "       [0.09591351, 0.09602994, 0.0997883 , 0.09900773, 0.11100361,\n",
       "        0.1016857 , 0.09782368, 0.10001057, 0.10524548, 0.09349147],\n",
       "       [0.09575948, 0.09619735, 0.09995307, 0.09856651, 0.11083112,\n",
       "        0.10208079, 0.09732594, 0.10032523, 0.10513687, 0.09382366],\n",
       "       [0.09593226, 0.0958754 , 0.09982686, 0.09894719, 0.11104408,\n",
       "        0.10161886, 0.0974529 , 0.09995244, 0.10529527, 0.09405474],\n",
       "       [0.09600949, 0.09599296, 0.09991827, 0.09846342, 0.11074667,\n",
       "        0.10168184, 0.09756665, 0.10047349, 0.10573367, 0.09341354],\n",
       "       [0.09551225, 0.0964067 , 0.09983276, 0.09878427, 0.11114087,\n",
       "        0.10181035, 0.09755692, 0.09999985, 0.10540853, 0.09354749],\n",
       "       [0.09595942, 0.09639741, 0.09965684, 0.09852249, 0.11125039,\n",
       "        0.10169324, 0.09755049, 0.0999535 , 0.10523696, 0.09377925],\n",
       "       [0.09630512, 0.09623516, 0.09962648, 0.09863856, 0.11092567,\n",
       "        0.10197526, 0.09740109, 0.09990799, 0.10520089, 0.09378376],\n",
       "       [0.09593943, 0.09655643, 0.09970342, 0.09905646, 0.11100398,\n",
       "        0.10130208, 0.09744531, 0.10018302, 0.10541311, 0.09339674],\n",
       "       [0.09586339, 0.09617716, 0.09993608, 0.09874457, 0.11113064,\n",
       "        0.10196628, 0.09738623, 0.09976139, 0.1052455 , 0.09378876],\n",
       "       [0.09604316, 0.09615492, 0.09944163, 0.09882359, 0.11119039,\n",
       "        0.10158807, 0.09771865, 0.10013618, 0.1052975 , 0.09360593],\n",
       "       [0.09621977, 0.09623311, 0.09992595, 0.09865599, 0.11080075,\n",
       "        0.1018366 , 0.09726607, 0.09994133, 0.10551131, 0.09360912],\n",
       "       [0.09594845, 0.09618489, 0.09997101, 0.09873228, 0.11134442,\n",
       "        0.10166269, 0.09748855, 0.09992027, 0.10545548, 0.09329197],\n",
       "       [0.09617423, 0.09622339, 0.09951929, 0.09847342, 0.11084342,\n",
       "        0.10192741, 0.09733217, 0.10051157, 0.10549119, 0.09350391],\n",
       "       [0.09577971, 0.09623908, 0.09960197, 0.09864614, 0.11096381,\n",
       "        0.10192435, 0.09766114, 0.10020329, 0.10521445, 0.09376607],\n",
       "       [0.09574732, 0.09630806, 0.09982754, 0.09902074, 0.11119162,\n",
       "        0.10218052, 0.09746193, 0.09990964, 0.10499104, 0.09336158],\n",
       "       [0.09588224, 0.09619503, 0.09958083, 0.09886137, 0.11131654,\n",
       "        0.10183295, 0.09756421, 0.09975106, 0.10551789, 0.09349788],\n",
       "       [0.09591547, 0.09634194, 0.09956196, 0.09852636, 0.1112336 ,\n",
       "        0.10226705, 0.09716613, 0.10010677, 0.10519185, 0.09368887],\n",
       "       [0.09613906, 0.09644332, 0.09964142, 0.09849939, 0.11110193,\n",
       "        0.10221863, 0.09718   , 0.10005325, 0.10504661, 0.0936764 ],\n",
       "       [0.09617023, 0.09614729, 0.09948524, 0.0988765 , 0.11092752,\n",
       "        0.10182013, 0.09748872, 0.1000653 , 0.1051984 , 0.09382066],\n",
       "       [0.09628841, 0.09614128, 0.09981148, 0.09846597, 0.11099233,\n",
       "        0.10200859, 0.09757742, 0.10022679, 0.10491554, 0.09357219],\n",
       "       [0.09624812, 0.09608092, 0.09978021, 0.09888986, 0.11101392,\n",
       "        0.10141606, 0.09744648, 0.0999648 , 0.10549908, 0.09366056],\n",
       "       [0.09612788, 0.09617321, 0.09979876, 0.09851248, 0.11084235,\n",
       "        0.10173602, 0.09756745, 0.10000673, 0.10557197, 0.09366316],\n",
       "       [0.09608066, 0.09592758, 0.09963166, 0.09838022, 0.11103924,\n",
       "        0.10176775, 0.09779032, 0.10011133, 0.1055994 , 0.09367183],\n",
       "       [0.09576639, 0.09640968, 0.09986296, 0.09865183, 0.11097761,\n",
       "        0.10204347, 0.09759733, 0.099781  , 0.10543918, 0.09347055],\n",
       "       [0.09611668, 0.09618735, 0.09987659, 0.09871632, 0.111551  ,\n",
       "        0.10185465, 0.09733139, 0.0998784 , 0.10483254, 0.09365509],\n",
       "       [0.09587411, 0.09626807, 0.09974226, 0.09830615, 0.11108832,\n",
       "        0.10186275, 0.09765392, 0.09976791, 0.10568225, 0.09375427],\n",
       "       [0.09588592, 0.09624839, 0.09998731, 0.0986826 , 0.11077398,\n",
       "        0.10197011, 0.09742418, 0.10011909, 0.10508722, 0.09382119],\n",
       "       [0.09577591, 0.09617699, 0.09985242, 0.09865513, 0.11069324,\n",
       "        0.10190734, 0.09755767, 0.10022129, 0.10568887, 0.09347113],\n",
       "       [0.09621318, 0.09640523, 0.09985663, 0.0987986 , 0.11109411,\n",
       "        0.10182506, 0.09741464, 0.10005779, 0.10468319, 0.09365157],\n",
       "       [0.09588692, 0.09613942, 0.09980551, 0.09862747, 0.11126195,\n",
       "        0.10200466, 0.09723405, 0.09998976, 0.10550452, 0.09354574],\n",
       "       [0.0963479 , 0.0961992 , 0.09957045, 0.09875137, 0.11093252,\n",
       "        0.10192219, 0.09750028, 0.09963736, 0.10567582, 0.09346291],\n",
       "       [0.09641038, 0.09609182, 0.09953667, 0.09863412, 0.11094224,\n",
       "        0.10163398, 0.09773974, 0.10000238, 0.10525131, 0.09375736],\n",
       "       [0.09568886, 0.09593676, 0.09976838, 0.09897146, 0.11071434,\n",
       "        0.10187656, 0.09757746, 0.10055464, 0.1054813 , 0.09343026],\n",
       "       [0.0962238 , 0.09632207, 0.09975295, 0.09874316, 0.11091222,\n",
       "        0.10187447, 0.09746639, 0.10006846, 0.10490486, 0.09373161],\n",
       "       [0.09575009, 0.09593964, 0.09998492, 0.09849795, 0.11112385,\n",
       "        0.10194387, 0.09744108, 0.10017127, 0.1057039 , 0.09344342],\n",
       "       [0.09606853, 0.09626977, 0.0995117 , 0.09885307, 0.1109335 ,\n",
       "        0.10181582, 0.09760264, 0.09976276, 0.10511655, 0.09406567],\n",
       "       [0.09622892, 0.09629022, 0.09959835, 0.09892026, 0.11073164,\n",
       "        0.10171589, 0.09756932, 0.1002753 , 0.1052946 , 0.09337549],\n",
       "       [0.09621132, 0.09620799, 0.09988799, 0.09883982, 0.11076891,\n",
       "        0.10175303, 0.09735267, 0.09974943, 0.10556563, 0.09366322],\n",
       "       [0.09576891, 0.09622808, 0.09997449, 0.09893943, 0.1110525 ,\n",
       "        0.10180829, 0.09750273, 0.09985717, 0.10530795, 0.09356046],\n",
       "       [0.09581071, 0.09639519, 0.10012862, 0.09859991, 0.11081749,\n",
       "        0.10186339, 0.09758646, 0.10002974, 0.10515888, 0.09360962],\n",
       "       [0.09576023, 0.0960468 , 0.09990674, 0.09844915, 0.1113436 ,\n",
       "        0.1020685 , 0.09739623, 0.10019066, 0.10528092, 0.09355717],\n",
       "       [0.09572702, 0.09616294, 0.09948493, 0.09843627, 0.11133733,\n",
       "        0.10220862, 0.09772605, 0.10033372, 0.10524938, 0.09333373],\n",
       "       [0.09581926, 0.09620407, 0.09985984, 0.09853571, 0.11138071,\n",
       "        0.10216415, 0.09724673, 0.09986658, 0.10525323, 0.09366971],\n",
       "       [0.09593372, 0.096125  , 0.09974212, 0.09841762, 0.11122464,\n",
       "        0.10222837, 0.09750907, 0.10006518, 0.10529989, 0.09345439],\n",
       "       [0.09619508, 0.09605397, 0.09937364, 0.09894881, 0.11129101,\n",
       "        0.10192522, 0.09748676, 0.09999369, 0.10507723, 0.09365459],\n",
       "       [0.09591542, 0.09636114, 0.09974533, 0.09860237, 0.1108729 ,\n",
       "        0.1017096 , 0.09719279, 0.10041907, 0.10577316, 0.09340823],\n",
       "       [0.09597519, 0.09630167, 0.09950432, 0.09886203, 0.11125281,\n",
       "        0.10186432, 0.09764473, 0.10003014, 0.10509777, 0.09346701],\n",
       "       [0.0962975 , 0.09622433, 0.09965193, 0.09923195, 0.11060296,\n",
       "        0.10186859, 0.0974391 , 0.09989941, 0.10508185, 0.09370239],\n",
       "       [0.0960298 , 0.09602968, 0.09963566, 0.09897457, 0.11098152,\n",
       "        0.10168309, 0.09756286, 0.10018564, 0.10522942, 0.09368777],\n",
       "       [0.09600608, 0.09629267, 0.09960432, 0.09888634, 0.11060774,\n",
       "        0.10161853, 0.09743798, 0.10022827, 0.10559135, 0.09372672],\n",
       "       [0.09582744, 0.09596211, 0.09971266, 0.09840012, 0.11110791,\n",
       "        0.10215819, 0.09753896, 0.10022353, 0.10549875, 0.09357033],\n",
       "       [0.09612411, 0.09622035, 0.09977143, 0.09889156, 0.11047664,\n",
       "        0.10178785, 0.09762319, 0.1002547 , 0.1052313 , 0.09361886],\n",
       "       [0.09581433, 0.0964081 , 0.09990218, 0.09861881, 0.11053758,\n",
       "        0.10180183, 0.09755601, 0.10010584, 0.10552015, 0.09373518],\n",
       "       [0.09615046, 0.09615196, 0.09967084, 0.09835665, 0.11099785,\n",
       "        0.10177727, 0.0975824 , 0.10016867, 0.10548201, 0.09366188],\n",
       "       [0.09598095, 0.0965306 , 0.09960996, 0.09874885, 0.11107181,\n",
       "        0.10202809, 0.09733709, 0.100125  , 0.10527189, 0.09329575],\n",
       "       [0.09604919, 0.09635225, 0.09956817, 0.09881708, 0.11100591,\n",
       "        0.10182116, 0.09723394, 0.10008735, 0.10526369, 0.09380125],\n",
       "       [0.09612284, 0.09624661, 0.09977362, 0.09890767, 0.11100021,\n",
       "        0.10175283, 0.09755021, 0.09994896, 0.10542041, 0.09327665],\n",
       "       [0.09568215, 0.09632025, 0.09997645, 0.09864465, 0.11089796,\n",
       "        0.10226295, 0.09741572, 0.09972396, 0.1053496 , 0.09372633],\n",
       "       [0.09618972, 0.09606458, 0.09968181, 0.09862565, 0.1112052 ,\n",
       "        0.10197746, 0.09743656, 0.10036784, 0.10512887, 0.0933223 ],\n",
       "       [0.09591073, 0.09630492, 0.09986521, 0.09878901, 0.11094529,\n",
       "        0.10154904, 0.097426  , 0.10001367, 0.10563931, 0.09355683],\n",
       "       [0.09634713, 0.09656626, 0.09933924, 0.09862792, 0.11080047,\n",
       "        0.10164949, 0.09745106, 0.1000676 , 0.10532938, 0.09382144],\n",
       "       [0.09600502, 0.09642282, 0.09950595, 0.09856637, 0.11094733,\n",
       "        0.10175128, 0.09789744, 0.09983289, 0.1052835 , 0.0937874 ],\n",
       "       [0.09607425, 0.09606096, 0.09973196, 0.09860913, 0.11106252,\n",
       "        0.10197555, 0.09767738, 0.09959455, 0.10562941, 0.09358427],\n",
       "       [0.09587773, 0.09615984, 0.09971067, 0.09892123, 0.11109082,\n",
       "        0.10188533, 0.09755802, 0.09994944, 0.10531556, 0.09353136]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # ダミーの入力データ(100枚分)\n",
    "t = np.random.rand(100, 10)  # ダミーの正解ラベル(100枚分)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 勾配を計算  5分くらいかかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['W1'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['W2'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['b2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 ミニバッチ学習の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "#iters_num = 10000\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-17 11:57:03.417784 0 start\n",
      "2021-11-17 11:57:03.588130 0 calculating gradient..\n",
      "2021-11-17 11:57:38.746525 0 updating params..\n",
      "2021-11-17 11:57:38.748177 0 recording loss..\n",
      "2021-11-17 11:57:38.750788 1 start\n",
      "2021-11-17 11:57:38.751992 1 calculating gradient..\n",
      "2021-11-17 11:58:18.206202 1 updating params..\n",
      "2021-11-17 11:58:18.207569 1 recording loss..\n",
      "2021-11-17 11:58:18.209202 2 start\n",
      "2021-11-17 11:58:18.210329 2 calculating gradient..\n",
      "2021-11-17 11:58:56.034166 2 updating params..\n",
      "2021-11-17 11:58:56.035545 2 recording loss..\n",
      "2021-11-17 11:58:56.036913 3 start\n",
      "2021-11-17 11:58:56.038012 3 calculating gradient..\n",
      "2021-11-17 11:59:32.658367 3 updating params..\n",
      "2021-11-17 11:59:32.659685 3 recording loss..\n",
      "2021-11-17 11:59:32.661057 4 start\n",
      "2021-11-17 11:59:32.662023 4 calculating gradient..\n",
      "2021-11-17 12:00:10.456674 4 updating params..\n",
      "2021-11-17 12:00:10.458641 4 recording loss..\n",
      "2021-11-17 12:00:10.461041 5 start\n",
      "2021-11-17 12:00:10.462192 5 calculating gradient..\n",
      "2021-11-17 12:00:52.089463 5 updating params..\n",
      "2021-11-17 12:00:52.091182 5 recording loss..\n",
      "2021-11-17 12:00:52.092817 6 start\n",
      "2021-11-17 12:00:52.094166 6 calculating gradient..\n",
      "2021-11-17 12:01:33.940131 6 updating params..\n",
      "2021-11-17 12:01:33.941644 6 recording loss..\n",
      "2021-11-17 12:01:33.943214 7 start\n",
      "2021-11-17 12:01:33.944571 7 calculating gradient..\n",
      "2021-11-17 12:02:18.428286 7 updating params..\n",
      "2021-11-17 12:02:18.430072 7 recording loss..\n",
      "2021-11-17 12:02:18.434629 8 start\n",
      "2021-11-17 12:02:18.436376 8 calculating gradient..\n",
      "2021-11-17 12:03:03.007136 8 updating params..\n",
      "2021-11-17 12:03:03.008710 8 recording loss..\n",
      "2021-11-17 12:03:03.010525 9 start\n",
      "2021-11-17 12:03:03.011872 9 calculating gradient..\n",
      "2021-11-17 12:03:46.873806 9 updating params..\n",
      "2021-11-17 12:03:46.875267 9 recording loss..\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "for i in range(iters_num):\n",
    "    print(datetime.now(),i,\"start\")\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    print(datetime.now(),i, \"calculating gradient..\")\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = nework.gradient(..) # 高速版\n",
    "    \n",
    "    # パラメータの更新\n",
    "    print(datetime.now(),i, \"updating params..\")\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    print(datetime.now(),i,\"recording loss..\")\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "# 10時間以上経っても終わらず。\n",
    "# 10ループなら11.5分程度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0:01:15 / loop\n",
    "- 75sec / loop\n",
    "- 10,000 loopだと\n",
    "  - 750,000 sec\n",
    "  - 10,250 min\n",
    "  - 170.8 hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 テストデータで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc__list = []\n",
    "# 1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "print(train_size, batch_size, iter_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-30 19:05:41.326270 start 0\n",
      "2018-05-30 19:05:41.327250 calculating gradient.. 0\n",
      "2018-05-30 19:06:59.391969 updating params.. 0\n",
      "2018-05-30 19:06:59.392604 recording loss.. 0\n",
      "train acc, test acc | 0.100983333333,0.1003\n",
      "2018-05-30 19:07:00.169775 start 1\n",
      "2018-05-30 19:07:00.170555 calculating gradient.. 1\n",
      "2018-05-30 19:08:18.287752 updating params.. 1\n",
      "2018-05-30 19:08:18.288498 recording loss.. 1\n",
      "2018-05-30 19:08:18.290601 start 2\n",
      "2018-05-30 19:08:18.291282 calculating gradient.. 2\n",
      "2018-05-30 19:09:35.520129 updating params.. 2\n",
      "2018-05-30 19:09:35.521195 recording loss.. 2\n",
      "2018-05-30 19:09:35.522597 start 3\n",
      "2018-05-30 19:09:35.523180 calculating gradient.. 3\n",
      "2018-05-30 19:10:53.301385 updating params.. 3\n",
      "2018-05-30 19:10:53.302177 recording loss.. 3\n",
      "2018-05-30 19:10:53.303760 start 4\n",
      "2018-05-30 19:10:53.304647 calculating gradient.. 4\n",
      "2018-05-30 19:12:10.802603 updating params.. 4\n",
      "2018-05-30 19:12:10.803771 recording loss.. 4\n",
      "2018-05-30 19:12:10.805571 start 5\n",
      "2018-05-30 19:12:10.806122 calculating gradient.. 5\n",
      "2018-05-30 19:13:27.519677 updating params.. 5\n",
      "2018-05-30 19:13:27.520534 recording loss.. 5\n",
      "2018-05-30 19:13:27.521930 start 6\n",
      "2018-05-30 19:13:27.522467 calculating gradient.. 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-cf946b97a4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 勾配の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"calculating gradient..\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# grad = nework.gradient(..) # 高速版\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hitachi500/gotowork/workspace/study-dl-from-scratch/chapter-04/common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# x:入力データ, t:教師データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# x:入力データ, t:教師データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hitachi500/gotowork/workspace/study-dl-from-scratch/chapter-04/common/functions.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/masaru/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2252\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/masaru/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "for i in range(iters_num):\n",
    "    print(datetime.now(),\"start\",i)\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    print(datetime.now(),\"calculating gradient..\",i)\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = nework.gradient(..) # 高速版\n",
    "    \n",
    "    # パラメータの更新\n",
    "    print(datetime.now(),\"updating params..\",i)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    print(datetime.now(),\"recording loss..\",i)\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1エポックごとに認識精度を計算\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc__list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \",\" +str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
