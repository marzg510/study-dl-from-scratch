{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから作るDeep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4章 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 学習アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 2層ニューラルネットワークのクラス\n",
    "\n",
    "2層のニューラルネットワーク（隠れ層が1層のニューラルネットワーク）を対象に、MNISTデータセットを使って学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/wd500/gotowork/workspace/study-dl-from-scratch/deep-learning-from-scratch/ch04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd ../deep-learning-from-scratch/ch04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    # このクラスで使用する変数\n",
    "    ## params : ニューラルネットワークのパラメータを保持するディクショナリ変数（インスタンス変数）\n",
    "    ##          params['W1']は1層目の重み、params['b1']は1層目のバイアス。\n",
    "    ##          params['W2']は2層目の重み、params['b2']は2層目のバイアス。\n",
    "    #\n",
    "    ## grads : 勾配を保持するディクショナリ変数（numerical_gradient()メソッドの返り値）\n",
    "    ##         grads['W1']は1層目の重みの勾配、grads['b1']は1層目のバイアスの勾配。\n",
    "    ##         grads['W2']は2層目の重みの勾配、grads['b2']は2層目のバイアスの勾配。\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一つ例を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # 今の層（入力層）のニューロン数×次の層（隠れ層）ニューロン数の行列になる\n",
    "print(net.params['b1'].shape) # 次の層（隠れ層）のニューロン数の行列になる\n",
    "print(net.params['W2'].shape) # 今の層（隠れ層）のニューロン数×次の層（出力層）のニューロン数の行列になる\n",
    "print(net.params['b2'].shape) # 次の層（出力層）のニューロン数の行列になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推論処理の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # ダミーの入力データ（１００枚分）\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30061326, 0.62200328, 0.06475344, ..., 0.90300161, 0.5716309 ,\n",
       "        0.22417583],\n",
       "       [0.09182456, 0.05423592, 0.56426271, ..., 0.85992835, 0.41810751,\n",
       "        0.67244417],\n",
       "       [0.55489101, 0.06015968, 0.28058285, ..., 0.16870425, 0.99879233,\n",
       "        0.22004696],\n",
       "       ...,\n",
       "       [0.59097383, 0.50058699, 0.64294482, ..., 0.97936782, 0.25013807,\n",
       "        0.23901405],\n",
       "       [0.48856935, 0.97073131, 0.45933961, ..., 0.70106356, 0.6889186 ,\n",
       "        0.37665326],\n",
       "       [0.08056828, 0.24073048, 0.71498597, ..., 0.3738099 , 0.94836747,\n",
       "        0.95779669]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10570701, 0.09614525, 0.10467622, 0.10313973, 0.10076265,\n",
       "        0.09742207, 0.09344511, 0.09944404, 0.0983814 , 0.10087652],\n",
       "       [0.10617315, 0.09628343, 0.10417493, 0.10325622, 0.10109316,\n",
       "        0.09711629, 0.09352671, 0.09963159, 0.09820417, 0.10054034],\n",
       "       [0.10548503, 0.09624465, 0.10464122, 0.10342485, 0.10137393,\n",
       "        0.09720527, 0.09342952, 0.0994257 , 0.09805932, 0.1007105 ],\n",
       "       [0.10578654, 0.09642624, 0.10427677, 0.10291702, 0.10118839,\n",
       "        0.09722556, 0.09333388, 0.09956796, 0.09846446, 0.10081319],\n",
       "       [0.10596829, 0.09611379, 0.104182  , 0.10311817, 0.10121774,\n",
       "        0.09725571, 0.09367103, 0.09946348, 0.0981176 , 0.10089219],\n",
       "       [0.10585183, 0.0964614 , 0.10435046, 0.10324876, 0.10130059,\n",
       "        0.09741796, 0.09306886, 0.09936615, 0.09801256, 0.10092143],\n",
       "       [0.10553975, 0.09634161, 0.10409341, 0.10279944, 0.10075795,\n",
       "        0.09774254, 0.0936028 , 0.09991489, 0.09832449, 0.10088311],\n",
       "       [0.1057161 , 0.09631046, 0.1045587 , 0.10335094, 0.10133773,\n",
       "        0.09691201, 0.09324301, 0.09949077, 0.09827741, 0.10080288],\n",
       "       [0.10552717, 0.09640023, 0.10439768, 0.10321845, 0.10160807,\n",
       "        0.09708266, 0.09356694, 0.09894938, 0.09852024, 0.10072919],\n",
       "       [0.10589155, 0.09685061, 0.10413644, 0.1029189 , 0.10095894,\n",
       "        0.09715833, 0.09352258, 0.09939269, 0.097957  , 0.10121296],\n",
       "       [0.1059565 , 0.09646905, 0.10412812, 0.1032376 , 0.10137209,\n",
       "        0.09703776, 0.09376934, 0.09898112, 0.09823243, 0.100816  ],\n",
       "       [0.10550084, 0.09650483, 0.10415707, 0.10335082, 0.1008157 ,\n",
       "        0.09706101, 0.09370196, 0.09943274, 0.09850405, 0.10097098],\n",
       "       [0.1059708 , 0.09593231, 0.10402329, 0.10317576, 0.10104048,\n",
       "        0.09752358, 0.0932377 , 0.09987375, 0.09831074, 0.10091159],\n",
       "       [0.10589409, 0.09637767, 0.10437223, 0.10362815, 0.10104791,\n",
       "        0.0970905 , 0.09325075, 0.09947462, 0.09830728, 0.10055681],\n",
       "       [0.10559543, 0.09636803, 0.10452207, 0.1032181 , 0.10163703,\n",
       "        0.0973616 , 0.09334942, 0.09907095, 0.09840212, 0.10047525],\n",
       "       [0.10593496, 0.096146  , 0.10459531, 0.10345641, 0.10092629,\n",
       "        0.0974526 , 0.09348696, 0.09940515, 0.09810847, 0.10048783],\n",
       "       [0.10579312, 0.0964628 , 0.10459791, 0.10294538, 0.10117012,\n",
       "        0.09716834, 0.09356604, 0.0991766 , 0.09836058, 0.10075909],\n",
       "       [0.10567434, 0.09664262, 0.1042133 , 0.10307871, 0.10144297,\n",
       "        0.09719829, 0.09353976, 0.09949341, 0.09796388, 0.10075272],\n",
       "       [0.10658968, 0.09606961, 0.10436318, 0.10282739, 0.10123209,\n",
       "        0.09730164, 0.09337106, 0.09941041, 0.09814059, 0.10069435],\n",
       "       [0.10573638, 0.09650567, 0.10430043, 0.10330794, 0.10122395,\n",
       "        0.097583  , 0.09342642, 0.09937296, 0.09791568, 0.10062756],\n",
       "       [0.10574111, 0.09659341, 0.10401721, 0.10315947, 0.10104007,\n",
       "        0.09726723, 0.09348157, 0.09955331, 0.09825562, 0.100891  ],\n",
       "       [0.10574102, 0.09631198, 0.10429539, 0.10354574, 0.101304  ,\n",
       "        0.09731368, 0.09325714, 0.09931632, 0.09809942, 0.1008153 ],\n",
       "       [0.10573883, 0.09651637, 0.10438637, 0.10334156, 0.10103404,\n",
       "        0.09700753, 0.09368171, 0.09945548, 0.09813908, 0.10069905],\n",
       "       [0.10534518, 0.09615773, 0.10431354, 0.10351361, 0.10097794,\n",
       "        0.09712742, 0.09349642, 0.09960556, 0.09841828, 0.1010443 ],\n",
       "       [0.10564716, 0.09640086, 0.10442867, 0.10315755, 0.10119617,\n",
       "        0.09718581, 0.09379964, 0.09937801, 0.0980743 , 0.10073183],\n",
       "       [0.10600422, 0.09657326, 0.1040748 , 0.10301849, 0.10121739,\n",
       "        0.09710507, 0.09360312, 0.09930933, 0.0981988 , 0.10089552],\n",
       "       [0.10584729, 0.09640672, 0.10448508, 0.10318064, 0.1013129 ,\n",
       "        0.09735833, 0.09323313, 0.09901682, 0.09838829, 0.10077079],\n",
       "       [0.10594891, 0.09665113, 0.10419749, 0.10306716, 0.10131633,\n",
       "        0.09727753, 0.09343797, 0.09933209, 0.0979605 , 0.1008109 ],\n",
       "       [0.10606027, 0.09622764, 0.10456278, 0.1032719 , 0.10124102,\n",
       "        0.09747295, 0.09328771, 0.0992866 , 0.09813543, 0.1004537 ],\n",
       "       [0.10596465, 0.09648459, 0.1047191 , 0.1030426 , 0.10129005,\n",
       "        0.09694032, 0.09341119, 0.09922272, 0.09806446, 0.10086033],\n",
       "       [0.10558126, 0.09618603, 0.10440905, 0.10290882, 0.10129096,\n",
       "        0.09766573, 0.09368136, 0.09918181, 0.09816005, 0.10093493],\n",
       "       [0.10578945, 0.09645912, 0.1046006 , 0.10285965, 0.10101196,\n",
       "        0.09734255, 0.09376296, 0.09915916, 0.09824762, 0.10076692],\n",
       "       [0.10593274, 0.09578919, 0.10457056, 0.10329589, 0.10102628,\n",
       "        0.09739866, 0.09343753, 0.09978993, 0.09812276, 0.10063647],\n",
       "       [0.10582965, 0.09636296, 0.10432832, 0.1034172 , 0.10104335,\n",
       "        0.09742988, 0.09362096, 0.09909396, 0.0980729 , 0.10080082],\n",
       "       [0.10580177, 0.09644109, 0.10398722, 0.10327568, 0.10125572,\n",
       "        0.09736527, 0.09329205, 0.09927592, 0.09841351, 0.10089177],\n",
       "       [0.10557094, 0.09648991, 0.10415822, 0.10327148, 0.10141166,\n",
       "        0.09714968, 0.09370959, 0.09945083, 0.09804997, 0.10073772],\n",
       "       [0.10578881, 0.09667553, 0.10429434, 0.10312011, 0.10119902,\n",
       "        0.0972026 , 0.0936961 , 0.09919896, 0.09805275, 0.10077178],\n",
       "       [0.10571824, 0.09642959, 0.10402401, 0.10289038, 0.10092444,\n",
       "        0.09725633, 0.09386389, 0.09966842, 0.09830252, 0.10092219],\n",
       "       [0.10581137, 0.09637728, 0.10429677, 0.10340743, 0.10104774,\n",
       "        0.09682964, 0.09372466, 0.09960972, 0.0981494 , 0.10074599],\n",
       "       [0.10562129, 0.0966431 , 0.10453891, 0.10326579, 0.10142324,\n",
       "        0.09719697, 0.0936464 , 0.0992049 , 0.0978728 , 0.1005866 ],\n",
       "       [0.10582924, 0.09628656, 0.10472948, 0.10309797, 0.10125645,\n",
       "        0.09705625, 0.0934782 , 0.09907881, 0.09844256, 0.1007445 ],\n",
       "       [0.10593636, 0.09626635, 0.10423987, 0.10329375, 0.10085485,\n",
       "        0.09719924, 0.09364114, 0.09951055, 0.09814423, 0.10091366],\n",
       "       [0.10589032, 0.0965648 , 0.10420888, 0.10305658, 0.10111348,\n",
       "        0.09715508, 0.0936234 , 0.09913376, 0.09868141, 0.10057229],\n",
       "       [0.10543875, 0.09640276, 0.10447684, 0.10352363, 0.1010227 ,\n",
       "        0.09722977, 0.09353391, 0.09936395, 0.09850452, 0.10050318],\n",
       "       [0.10583578, 0.09622424, 0.10414382, 0.10327316, 0.10110191,\n",
       "        0.09721692, 0.09368588, 0.09944875, 0.09827177, 0.10079777],\n",
       "       [0.10567942, 0.0963423 , 0.10428116, 0.10310987, 0.10110547,\n",
       "        0.09709649, 0.09344309, 0.09964265, 0.09851523, 0.10078432],\n",
       "       [0.10578881, 0.09639234, 0.10448926, 0.10323083, 0.10089664,\n",
       "        0.09697026, 0.09365615, 0.09975643, 0.09823497, 0.10058432],\n",
       "       [0.10574497, 0.09636053, 0.10410064, 0.10318769, 0.10101304,\n",
       "        0.09732014, 0.09362149, 0.09952313, 0.09825069, 0.1008777 ],\n",
       "       [0.10608038, 0.09603745, 0.10416167, 0.10337122, 0.10102316,\n",
       "        0.09729165, 0.0934364 , 0.09981189, 0.09812455, 0.10066163],\n",
       "       [0.10591653, 0.09637926, 0.10443669, 0.10327529, 0.10105431,\n",
       "        0.09716198, 0.09369947, 0.09923565, 0.09837679, 0.10046403],\n",
       "       [0.10590615, 0.09611316, 0.10424585, 0.10306782, 0.101138  ,\n",
       "        0.09746436, 0.09374212, 0.09930966, 0.09809164, 0.10092123],\n",
       "       [0.1062681 , 0.09641231, 0.10415326, 0.10320234, 0.10124555,\n",
       "        0.09710089, 0.09370716, 0.09930804, 0.0980541 , 0.10054825],\n",
       "       [0.10584555, 0.09648254, 0.10444083, 0.10327499, 0.10113321,\n",
       "        0.09715175, 0.09346304, 0.09937416, 0.09807268, 0.10076124],\n",
       "       [0.10576824, 0.09664153, 0.10404194, 0.10357352, 0.10136446,\n",
       "        0.0970797 , 0.09342974, 0.09933431, 0.09802061, 0.10074595],\n",
       "       [0.1058879 , 0.09597837, 0.10428033, 0.10319866, 0.10097458,\n",
       "        0.09728059, 0.09331926, 0.09947723, 0.09849554, 0.10110754],\n",
       "       [0.10594546, 0.09643588, 0.10446459, 0.10350086, 0.10095005,\n",
       "        0.09682276, 0.09343739, 0.09903801, 0.09850213, 0.10090287],\n",
       "       [0.10596887, 0.09628726, 0.10444859, 0.10299511, 0.10093125,\n",
       "        0.09737445, 0.09348313, 0.09928851, 0.09827838, 0.10094445],\n",
       "       [0.10610642, 0.09645267, 0.10418646, 0.10285503, 0.10085592,\n",
       "        0.09742068, 0.09373205, 0.09955944, 0.09801889, 0.10081243],\n",
       "       [0.10574549, 0.09633124, 0.10413014, 0.10315228, 0.10103667,\n",
       "        0.09696894, 0.09375203, 0.09971276, 0.09818672, 0.10098374],\n",
       "       [0.10604111, 0.09583819, 0.10420142, 0.10324533, 0.10091966,\n",
       "        0.09707587, 0.09378431, 0.09948565, 0.09844877, 0.10095968],\n",
       "       [0.10621818, 0.09638346, 0.10427733, 0.10280201, 0.10136716,\n",
       "        0.09709318, 0.0935547 , 0.09939041, 0.09809637, 0.10081719],\n",
       "       [0.10555194, 0.09677393, 0.1042174 , 0.10304341, 0.10116044,\n",
       "        0.09708132, 0.09359397, 0.09937857, 0.09837694, 0.10082208],\n",
       "       [0.1057121 , 0.09633819, 0.10437012, 0.10328076, 0.10112934,\n",
       "        0.09701676, 0.09368648, 0.09954821, 0.09791914, 0.10099891],\n",
       "       [0.10585525, 0.09628093, 0.10435318, 0.10325081, 0.10133095,\n",
       "        0.09744854, 0.09314001, 0.09936176, 0.09825814, 0.10072043],\n",
       "       [0.10617244, 0.09630394, 0.10438833, 0.10303217, 0.1012818 ,\n",
       "        0.09716994, 0.0935581 , 0.09949727, 0.09802712, 0.1005689 ],\n",
       "       [0.1060774 , 0.09638338, 0.10416262, 0.10307797, 0.10108223,\n",
       "        0.09727184, 0.09337537, 0.09943603, 0.09801584, 0.10111731],\n",
       "       [0.10607754, 0.09644822, 0.10436382, 0.10296986, 0.10128678,\n",
       "        0.09724443, 0.09344562, 0.0991688 , 0.09841284, 0.1005821 ],\n",
       "       [0.10563736, 0.09629845, 0.10439424, 0.10363016, 0.10103278,\n",
       "        0.09745661, 0.09337819, 0.09905916, 0.09845028, 0.10066278],\n",
       "       [0.1058231 , 0.09648945, 0.10424775, 0.10272364, 0.10136918,\n",
       "        0.09725076, 0.09379134, 0.0991753 , 0.09822114, 0.10090834],\n",
       "       [0.10586243, 0.09616437, 0.10442105, 0.10306705, 0.10152418,\n",
       "        0.09746819, 0.09351119, 0.09920395, 0.09821311, 0.10056447],\n",
       "       [0.10586833, 0.09633408, 0.10450748, 0.10302853, 0.10094461,\n",
       "        0.09717597, 0.09390458, 0.09889274, 0.09850929, 0.10083441],\n",
       "       [0.10573085, 0.09631365, 0.10423745, 0.10317156, 0.10103719,\n",
       "        0.0973932 , 0.09355264, 0.09950402, 0.09809214, 0.10096728],\n",
       "       [0.10556162, 0.09614358, 0.10422867, 0.10308794, 0.1016254 ,\n",
       "        0.09734644, 0.09409842, 0.09943873, 0.09798027, 0.10048893],\n",
       "       [0.10587   , 0.09632711, 0.10418253, 0.10325247, 0.10088772,\n",
       "        0.09744034, 0.0935989 , 0.09927427, 0.09838785, 0.10077881],\n",
       "       [0.1055192 , 0.09606373, 0.1041108 , 0.10358275, 0.10132222,\n",
       "        0.09713575, 0.09382356, 0.09955895, 0.09802934, 0.10085371],\n",
       "       [0.10605778, 0.09630609, 0.10437308, 0.10318474, 0.10116138,\n",
       "        0.09707354, 0.09339727, 0.09976144, 0.09793055, 0.10075414],\n",
       "       [0.10552682, 0.09595601, 0.10416623, 0.10332786, 0.10107965,\n",
       "        0.09758292, 0.09356583, 0.09956381, 0.09820526, 0.10102562],\n",
       "       [0.1056936 , 0.09648034, 0.1044361 , 0.10322178, 0.10101968,\n",
       "        0.09739565, 0.09366699, 0.09916166, 0.09818087, 0.10074332],\n",
       "       [0.10623442, 0.09633996, 0.10414049, 0.10289482, 0.10111166,\n",
       "        0.09733691, 0.09376801, 0.09936985, 0.09809956, 0.10070432],\n",
       "       [0.10585701, 0.09651406, 0.10405173, 0.10325205, 0.10112712,\n",
       "        0.09690482, 0.09345822, 0.09933901, 0.09847856, 0.10101743],\n",
       "       [0.10596735, 0.09632907, 0.1045368 , 0.103327  , 0.10128755,\n",
       "        0.09708272, 0.09342258, 0.09922874, 0.09774708, 0.10107111],\n",
       "       [0.10596111, 0.09646614, 0.10459531, 0.10351111, 0.10090024,\n",
       "        0.09706294, 0.09354   , 0.0991609 , 0.09815829, 0.10064397],\n",
       "       [0.1056912 , 0.09659355, 0.10404566, 0.10313798, 0.10117361,\n",
       "        0.09728463, 0.0936831 , 0.0991753 , 0.0982817 , 0.10093326],\n",
       "       [0.10594841, 0.09635581, 0.10429906, 0.10316068, 0.10118055,\n",
       "        0.09724598, 0.09345657, 0.09948429, 0.09801893, 0.10084972],\n",
       "       [0.10641439, 0.09606203, 0.10461644, 0.10334907, 0.10119679,\n",
       "        0.09702048, 0.09311677, 0.09922216, 0.09822863, 0.10077324],\n",
       "       [0.10597702, 0.09656734, 0.10429557, 0.10317483, 0.10076016,\n",
       "        0.09707782, 0.09374556, 0.09949993, 0.09814984, 0.10075192],\n",
       "       [0.10616903, 0.09618875, 0.10441162, 0.10317295, 0.10116383,\n",
       "        0.09698488, 0.09353623, 0.09943359, 0.0981957 , 0.10074342],\n",
       "       [0.10601464, 0.09623838, 0.10437583, 0.10299377, 0.10086528,\n",
       "        0.09737254, 0.09380923, 0.09927197, 0.09845367, 0.1006047 ],\n",
       "       [0.10594284, 0.09628743, 0.10414753, 0.10283905, 0.10085368,\n",
       "        0.09741275, 0.0940062 , 0.09947824, 0.0982711 , 0.10076118],\n",
       "       [0.10582054, 0.09665335, 0.10437291, 0.1027994 , 0.10073199,\n",
       "        0.0969904 , 0.09396383, 0.09937121, 0.09842668, 0.10086969],\n",
       "       [0.1059825 , 0.09627468, 0.10432798, 0.10315693, 0.10108092,\n",
       "        0.09732312, 0.09350247, 0.09924608, 0.09827229, 0.10083303],\n",
       "       [0.10594099, 0.0963001 , 0.10442622, 0.1033619 , 0.10111489,\n",
       "        0.09711764, 0.0935853 , 0.09927852, 0.09805516, 0.10081929],\n",
       "       [0.10567393, 0.09637041, 0.10450621, 0.10318309, 0.10097773,\n",
       "        0.09721948, 0.09344912, 0.09959648, 0.09817179, 0.10085175],\n",
       "       [0.10592072, 0.0967426 , 0.1043903 , 0.10298585, 0.10139932,\n",
       "        0.09723762, 0.09330496, 0.09899061, 0.098165  , 0.10086302],\n",
       "       [0.10598967, 0.09582446, 0.10418344, 0.10332387, 0.10118106,\n",
       "        0.09728467, 0.0935172 , 0.09976067, 0.0982761 , 0.10065886],\n",
       "       [0.1055651 , 0.09655569, 0.1041806 , 0.10292173, 0.10141154,\n",
       "        0.09716313, 0.09354197, 0.09932111, 0.09827638, 0.10106273],\n",
       "       [0.1060976 , 0.09633332, 0.1043111 , 0.1032761 , 0.10122678,\n",
       "        0.09730853, 0.09352668, 0.09889695, 0.09803287, 0.10099006],\n",
       "       [0.10608592, 0.09612538, 0.10419671, 0.10300281, 0.10093874,\n",
       "        0.09728652, 0.09373256, 0.09956225, 0.09808266, 0.10098645],\n",
       "       [0.10600979, 0.09626938, 0.10388322, 0.10332202, 0.10136796,\n",
       "        0.09714731, 0.09351496, 0.09917389, 0.09847699, 0.10083448],\n",
       "       [0.10560507, 0.0964658 , 0.10429894, 0.1034285 , 0.10094675,\n",
       "        0.09728722, 0.09353036, 0.09942948, 0.09839546, 0.10061243]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # ダミーの入力データ(100枚分)\n",
    "t = np.random.rand(100, 10)  # ダミーの正解ラベル(100枚分)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 勾配を計算  5分くらいかかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['W1'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['W2'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['b2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 ミニバッチ学習の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "#iters_num = 10000\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-16 22:47:51.941923 0 start\n",
      "2021-11-16 22:47:51.944228 0 calculating gradient..\n",
      "2021-11-16 22:49:00.604404 0 updating params..\n",
      "2021-11-16 22:49:00.605454 0 recording loss..\n",
      "2021-11-16 22:49:00.607454 1 start\n",
      "2021-11-16 22:49:00.608317 1 calculating gradient..\n",
      "2021-11-16 22:50:13.435413 1 updating params..\n",
      "2021-11-16 22:50:13.436402 1 recording loss..\n",
      "2021-11-16 22:50:13.437770 2 start\n",
      "2021-11-16 22:50:13.438645 2 calculating gradient..\n",
      "2021-11-16 22:51:22.218434 2 updating params..\n",
      "2021-11-16 22:51:22.220093 2 recording loss..\n",
      "2021-11-16 22:51:22.221467 3 start\n",
      "2021-11-16 22:51:22.222178 3 calculating gradient..\n",
      "2021-11-16 22:52:30.279966 3 updating params..\n",
      "2021-11-16 22:52:30.280915 3 recording loss..\n",
      "2021-11-16 22:52:30.282519 4 start\n",
      "2021-11-16 22:52:30.283709 4 calculating gradient..\n",
      "2021-11-16 22:53:38.526185 4 updating params..\n",
      "2021-11-16 22:53:38.527551 4 recording loss..\n",
      "2021-11-16 22:53:38.529349 5 start\n",
      "2021-11-16 22:53:38.530301 5 calculating gradient..\n",
      "2021-11-16 22:54:49.718111 5 updating params..\n",
      "2021-11-16 22:54:49.719150 5 recording loss..\n",
      "2021-11-16 22:54:49.720425 6 start\n",
      "2021-11-16 22:54:49.721073 6 calculating gradient..\n",
      "2021-11-16 22:55:57.183386 6 updating params..\n",
      "2021-11-16 22:55:57.184462 6 recording loss..\n",
      "2021-11-16 22:55:57.188111 7 start\n",
      "2021-11-16 22:55:57.189018 7 calculating gradient..\n",
      "2021-11-16 22:57:03.245959 7 updating params..\n",
      "2021-11-16 22:57:03.246896 7 recording loss..\n",
      "2021-11-16 22:57:03.248180 8 start\n",
      "2021-11-16 22:57:03.248880 8 calculating gradient..\n",
      "2021-11-16 22:58:15.617858 8 updating params..\n",
      "2021-11-16 22:58:15.618887 8 recording loss..\n",
      "2021-11-16 22:58:15.620328 9 start\n",
      "2021-11-16 22:58:15.621037 9 calculating gradient..\n",
      "2021-11-16 22:59:27.050603 9 updating params..\n",
      "2021-11-16 22:59:27.051552 9 recording loss..\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "for i in range(iters_num):\n",
    "    print(datetime.now(),i,\"start\")\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    print(datetime.now(),i, \"calculating gradient..\")\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = nework.gradient(..) # 高速版\n",
    "    \n",
    "    # パラメータの更新\n",
    "    print(datetime.now(),i, \"updating params..\")\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    print(datetime.now(),i,\"recording loss..\")\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "# 10時間以上経っても終わらず。\n",
    "# 10ループなら11.5分程度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0:01:15 / loop\n",
    "- 75sec / loop\n",
    "- 10,000 loopだと\n",
    "  - 750,000 sec\n",
    "  - 10,250 min\n",
    "  - 170.8 hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 テストデータで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc__list = []\n",
    "# 1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-30 19:05:41.326270 start 0\n",
      "2018-05-30 19:05:41.327250 calculating gradient.. 0\n",
      "2018-05-30 19:06:59.391969 updating params.. 0\n",
      "2018-05-30 19:06:59.392604 recording loss.. 0\n",
      "train acc, test acc | 0.100983333333,0.1003\n",
      "2018-05-30 19:07:00.169775 start 1\n",
      "2018-05-30 19:07:00.170555 calculating gradient.. 1\n",
      "2018-05-30 19:08:18.287752 updating params.. 1\n",
      "2018-05-30 19:08:18.288498 recording loss.. 1\n",
      "2018-05-30 19:08:18.290601 start 2\n",
      "2018-05-30 19:08:18.291282 calculating gradient.. 2\n",
      "2018-05-30 19:09:35.520129 updating params.. 2\n",
      "2018-05-30 19:09:35.521195 recording loss.. 2\n",
      "2018-05-30 19:09:35.522597 start 3\n",
      "2018-05-30 19:09:35.523180 calculating gradient.. 3\n",
      "2018-05-30 19:10:53.301385 updating params.. 3\n",
      "2018-05-30 19:10:53.302177 recording loss.. 3\n",
      "2018-05-30 19:10:53.303760 start 4\n",
      "2018-05-30 19:10:53.304647 calculating gradient.. 4\n",
      "2018-05-30 19:12:10.802603 updating params.. 4\n",
      "2018-05-30 19:12:10.803771 recording loss.. 4\n",
      "2018-05-30 19:12:10.805571 start 5\n",
      "2018-05-30 19:12:10.806122 calculating gradient.. 5\n",
      "2018-05-30 19:13:27.519677 updating params.. 5\n",
      "2018-05-30 19:13:27.520534 recording loss.. 5\n",
      "2018-05-30 19:13:27.521930 start 6\n",
      "2018-05-30 19:13:27.522467 calculating gradient.. 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-cf946b97a4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 勾配の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"calculating gradient..\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# grad = nework.gradient(..) # 高速版\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hitachi500/gotowork/workspace/study-dl-from-scratch/chapter-04/common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# x:入力データ, t:教師データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# x:入力データ, t:教師データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b6da1489fdd6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/hitachi500/gotowork/workspace/study-dl-from-scratch/chapter-04/common/functions.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/masaru/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2252\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/masaru/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "for i in range(iters_num):\n",
    "    print(datetime.now(),\"start\",i)\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    print(datetime.now(),\"calculating gradient..\",i)\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = nework.gradient(..) # 高速版\n",
    "    \n",
    "    # パラメータの更新\n",
    "    print(datetime.now(),\"updating params..\",i)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    print(datetime.now(),\"recording loss..\",i)\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1エポックごとに認識精度を計算\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc__list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \",\" +str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
